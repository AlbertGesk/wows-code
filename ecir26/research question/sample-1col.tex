%% The first command in your LaTeX source must be the \documentclass command.
%%
%% Options:
%% twocolumn : Two column layout. Do not use twocolumn for papers submitted to CEUR-WS!
%% hf: enable header and footer.
\documentclass[
twocolumn,
% hf,
]{ceurart}

%%
%% One can fix some overfulls
\sloppy

%%
%% Minted listings support 
%% Need pygment <http://pygments.org/> <http://pypi.python.org/pypi/Pygments>
\usepackage{listings}
%% auto break lines
\lstset{breaklines=true}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% Rights management information.
%% CC-BY is default license.
\copyrightyear{2026}
\copyrightclause{Copyright for this paper by its authors.
  Use permitted under Creative Commons License Attribution 4.0
  International (CC BY 4.0).}

%%
%% This command is for the conference information
\conference{IR Lab Course, Winter Semester 2025}

%%
%% The "title" command
\title{A Comparative Study of Bo1 and RM3 Query Expansion using DPH Retrieval}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
\author[1]{Albert Gesk}[%
email=uk071960@uni-kassel.de,
url=https://github.com/AlbertGesk,
]
\author[1]{Lilian Erler}[%
email=uk076601@student.uni-kassel.de,
url=https://github.com/lilianerler,
]
\address[1]{University Kassel, Germany}


%% Footnotes
% \cortext[1]{Corresponding author.}
% \fntext[1]{These authors contributed equally.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  Query expansion via pseudo-relevance feedback is a well-established technique to improve 
  retrieval effectiveness in information retrieval systems. Among classical approaches, 
  Bo1 and RM3 are frequently used but exhibit varying effectiveness depending on the 
  retrieval setting. In this work, we empirically compare Bo1 and RM3 query expansion 
  within a controlled Retriever-Rewriter-Retriever pipeline using the DPH retrieval model. 
  Experiments are conducted on the \textit{radboud-validation-20251114-training} dataset 
  using nDCG@10 as evaluation metric. Statistical significance testing shows that neither 
  a significant difference nor a significant improvement in retrieval effectiveness can be 
  observed between Bo1 and RM3 under identical parameter settings.
\end{abstract}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\begin{keywords}
  Information Retrieval \sep
  Query Expansion \sep
  Bo1 \sep
  RM3 \sep
  PyTerrier
\end{keywords}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

Query expansion using pseudo-relevance feedback (PRF) is a classical approach to mitigate 
vocabulary mismatch in information retrieval (IR) systems. The mechanism of pseudo-relevance 
feedback (PRF) involves extracting expansion terms from the initial top-ranked documents to 
formulate a new query for a second retrieval stage \cite{Wang25}. Among commonly used PRF techniques, Bo1 and RM3
are widely adopted in traditional IR systems and toolkits.

\noindent Despite their popularity, the relative effectiveness of Bo1 and RM3 is known to be sensitive 
to retrieval models, parameter settings, and datasets. This motivates a controlled comparison 
of both methods under identical experimental conditions.

\noindent In this paper, we investigate the following research question: \textit{Does the choice of Bo1 versus RM3 query expansion lead to significant differences 
or improvements in retrieval effectiveness when applied within a DPH-based retrieval pipeline?}

\noindent Our contributions are:
(i) a controlled experimental comparison of Bo1 and RM3 query expansion using identical 
retrieval and feedback settings, and
(ii) a statistical analysis of their impact on nDCG@10 on the 
\textit{radboud-validation-20251114-training} dataset.

\section{Related Work}
Pseudo-relevance feedback (PRF) is a well-established technique in information retrieval 
to improve query effectiveness by automatically expanding the original query with terms 
extracted from top-ranked documents \cite{Wang25}.  

Previous studies have shown that RM3 provides robust improvements over baseline retrieval models
like BM25, particularly when the number of feedback documents and expansion terms are carefully
tuned. However, the quality of the expansion terms can be limited because of it's reliance on pseudo-
relevant documents. This happens especially in scenarios where the top-ranked documents are not 
truly relevant. \cite{10725314}

These findings motivate a controlled comparison between RM3 and Bo1, as they represent 
alternative pseudo-relevance feedback methods based on distinct term selection principles.
Bo1 is based on divergence-from-randomness (DRF) and selects terms that are statistically informative 
in the top-retrieved documents \cite{de2024performance}, while RM3 is a probabilistic relevance 
model \cite{10725314}.

Other works have explored hybrid or transformer-based retrieval pipelines that 
incorporate PRF or developed a demanding pipelines, such as ColBERT-PRF, which integrates
semantic pseudo-relevance feedback into dense retrieval models \cite{10.1145/3572405}. But controlled comparisons of 
classical Bo1 and RM3 within identical DPH-based pipelines remain limited.

This motivates our work to conduct a systematic and controlled evaluation of Bo1 and RM3 
under identical experimental conditions, assessing not only retrieval effectiveness but also 
statistical significance of the observed differences.

\section{Methodology}
This section depicts the experimental setup designed to answer the research question.

\subsection{Dataset and Preprocessing}
For the experiments, we utilized the \textit{radboud-validation-20251114-training} dataset. 
The indexing of the documents was based on the standard textual representation (\texttt{default\_text}) 
provided in the dataset. No additional preprocessing steps or filtering were applied, 
meaning only the standard tokenization performed by PyTerrier was used. 

\subsection{Experimental Design}
\label{subsec:exp}
The experiment use a retriever-rewriter-retriever pipeline as foundation. The DPH retrieval model \cite{DBLP:conf/ecir/Amati06} implemented via PyTerrier 
is setup with a thousand maximum number of results to return per query and with no metadata fields to return for each search result. 
The pseudo-relevance feedback models,both the Bo1 \cite{DBLP:phd/ethos/Amati03} and RM3 \cite{DBLP:conf/trec/JaleelACDLLSW04} are implemented via PyTerrier as well.
For both models, all parameters are remained at default, the number of feedback terms to use is set to ten and the number of feedback documents to use is set to three. 
The interpolation weight between the original query and the feedback model for RM3 is set to 0.6.

\noindent To evaluate the effectiveness of the retrieval pipelines, differences in per-topic 
nDCG@10 scores were assessed for statistical significance using a paired Student’s 
t-test. The significance level was set to $\alpha = 0.05$, and Bonferroni correction 
was applied to account for multiple comparisons.

\subsection{Hypotheses and Null-Hypotheses}
Based on the research question, two hypotheses and their corresponding null hypotheses were formulated:
\begin{itemize}
  \item $H^{}_{1}$ Given query expansion via pseudo-relevance feedback using the top 3 retrieved documents and 10 expansion terms, there is a statistically significant difference in nDCG@10 on radboud-validation-20251114-training between (i) the DPH-based retrieval pipeline described in subsection~\ref{subsec:exp} three with Bo1 query expansion and (ii) the same pipeline with RM3 query expansion. ($\alpha$=0.05)
  \item $H^{0}_{1}$ Given query expansion via pseudo-relevance feedback using the top 3 retrieved documents and 10 expansion terms, there is no statistically significant difference in nDCG@10 on radboud-validation-20251114-training between (i) the DPH-based retrieval pipeline described in subsection~\ref{subsec:exp} three with Bo1 query expansion and (ii) the same pipeline with RM3 query expansion. ($\alpha$=0.05) 
  \item $H^{}_{2}$ Given query expansion via pseudo-relevance feedback using the top 3 retrieved documents and 10 expansion terms, there is a statistically significant improvement in nDCG@10 on radboud-validation-20251114-training between (i) the DPH-based retrieval pipeline described in subsection~\ref{subsec:exp} three with Bo1 query expansion and (ii) the same pipeline with RM3 query expansion. ($\alpha$=0.05)
  \item $H^{0}_{2}$ Given query expansion via pseudo-relevance feedback using the top 3 retrieved documents and 10 expansion terms, there is no statistically significant improvement in nDCG@10 on radboud-validation-20251114-training between (i) the DPH-based retrieval pipeline described in subsection~\ref{subsec:exp} three with Bo1 query expansion and (ii) the same pipeline with RM3 query expansion. ($\alpha$=0.05)
\end{itemize}

\section{Results}
Table~\ref{tab:results} presents the retrieval effectiveness and statistical testing results.

The DPH-Bo1-DPH pipeline achieves a higher mean nDCG@10 score than DPH-RM3-DPH. 
However, Bo1 does not significantly outperform RM3. The obtained p-value (0.283) is 
substantially greater than the significance threshold of 0.05, indicating that the observed 
difference is likely due to random variation across topics. Consequently, we fail to reject the null hypotheses 
$H^{0}_{1}$ and $H^{0}_{2}$, and therefore find no statistical evidence to support the alternative hypotheses $H^{}_{1}$ and $H^{}_{2}$.

\begin{table}[t]
\centering
\caption{Retrieval effectiveness and statistical significance (nDCG@10).}
\label{tab:results}
\begin{tabular}{lccc}
\toprule
Method & nDCG@10 & p-value \\
\midrule
DPH-Bo1-DPH & 0.4947 & \multirow{2}{*}{0.283} \\
DPH-RM3-DPH & 0.4744 &   \\
\bottomrule
\end{tabular}
\end{table}

\section{Conclusion}
This study presented a controlled comparison of Bo1 and RM3 pseudo-relevance feedback methods within an 
identical DPH-based retriever–rewriter–retriever pipeline. Using three feedback documents and ten expansion terms, 
both approaches yielded comparable retrieval effectiveness on the \textit{radboud-validation-20251114-training} dataset. 
Although the DPH-Bo1-DPH pipeline achieved a slightly higher mean nDCG@10 score than DPH-RM3-DPH, statistical 
significance testing showed that this difference was not significant. Consequently, neither method can be 
considered superior under the examined experimental conditions.

These findings suggest that, when applied with identical parameters and retrieval models, the choice 
between Bo1 and RM3 has limited impact on retrieval effectiveness. Future work could explore broader 
parameter sweeps, alternative feedback depths, or different retrieval models to better understand under 
which conditions one method may consistently outperform the other.

%% The declaration on generative AI comes in effect
%% in Janary 2025. See also
%% https://ceur-ws.org/GenAI/Policy.html
\section*{Declaration on Generative AI}
 During the preparation of this work, the authors used ChatGPT-5.2 and LanguageTool in order to: Grammar and spelling check. After using these tools/services, the authors reviewed and edited the content as needed and take full responsibility for the publication’s content. 


\pagebreak
%%
%% Define the bibliography file to be used
\bibliography{sample-ceur}

\end{document}

%%
%% End of file

